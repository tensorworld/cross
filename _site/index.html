<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>CROSS</title>
    <meta name="description" content="Cross-layer Coordination and Optimization for Scalable and Sparse Tensor Networks (CROSS)">
    <meta name="author" content="Cross-layer Coordination and Optimization for Scalable and Sparse Tensor Networks (CROSS)">

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!-- <link rel="stylesheet" href="static/css/base.css"> -->
    <link rel="stylesheet" href="static/css/skeleton.css">
    <link rel="stylesheet" href="static/css/layout.css">
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

    <!--[if lt IE 9]>
    <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- <link rel="shortcut icon" href="static/img/favicon.ico"> -->
</head>

<body>
<div class="container">
    <!-- <header class="twelve columns">
        Cross-layer Coordination and Optimization for Scalable and Sparse Tensor Networks (CROSS)
    </header> -->

    <header class="four columns">
        <!-- <img src="static/img/ljj.jpg" class="me">
        <h1>Cross-layer Coordination and Optimization for Scalable and Sparse Tensor Networks (CROSS)</h1>
        <p>Computer Science</p>
        <h2>NCSU</h2> -->

        <nav class="navigation">
            <ul class="navigation-list">
            <li class="navigation-item">
              <a class="navigation-link" href="/index.html" data-popover>Home</a>
            </li>
            <li class="navigation-item">
              <a class="navigation-link" href="#activities" data-popover>Activities</a>
            </li>
            <li class="navigation-item">
              <a class="navigation-link" href="#projects" data-popover>Projects</a>
            </li> 
            <li class="navigation-item">
              <a class="navigation-link" href="#publication" data-popover>Publication</a>
            </li>            
            <li class="navigation-item">
              <a class="navigation-link" href="#description" data-popover>Description</a>
            </li>
            <li class="navigation-item">
              <a class="navigation-link" href="#team" data-popover>Team</a>
            </li> 
            <li class="navigation-item">
              <a class="navigation-link" href="#contact" data-popover>Contact</a>
            </li> 
            </ul>
        </nav>

        <p>
            Website: <a href="http://localhost:4000">http://localhost:4000</a><br/>
            Contact: <a href="">jiajia.li@ncsu.edu</a><br/>
        </p>

        </section>
    </header>

    <div class="twelve columns clearfix">

        <!DOCTYPE html>

<h2>NSF Collaborative Research: PPoSS: LARGE: Cross-layer Coordination and Optimization for Scalable and Sparse Tensor Networks (CROSS)</h2>

</br></br>

<section class="twelve columns" id="news">
    <h1>News</h1>
        <article>
        <ul>
          <li><strong>Nov 17-22:</strong> NSF PPoSS CROSS Project PI Meeting at <a href="https://sc24.supercomputing.org/">SC'25</a></li>        
        </ul>
    </article>
</section>

<section class="twelve columns" id="activities">
    <h1>Activities</h1>
    <article>
        <ul>
        	<li>
        		<p><strong>Sep 3, 6:</strong> NSF PPoSS CROSS Project Annual Meeting with Advisory Broad, Online (Meeting Report [<a href="https://drive.google.com/file/d/1bStLNX4csUG1MT3HD5mO2uGSCRCy_KWW/view?usp=sharing">HERE</a>])</p>
        	</li> 
            <li>
                <p><em><a href="https://fruitfly1026.github.io/static/files/xtensor-asplos24.html">XTensor 2024: 1st Workshop on Cross-stack Optimization of Tensor Methods</a>, San Diego, CA</em></p>
            </li>

    </article>
</section>


<section class="twelve columns" id="projects">
    <h1>Projects</h1>
    <article>
        <ul>
            <li>
                <header>
                    <p><em>CROSS: <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2316201&HistoricalAwards=false">Collaborative Research: PPoSS: LARGE: Cross-layer Coordination and Optimization for Scalable and Sparse Tensor Networks (CROSS)</a></em></p>
                </header>
                <p>
                    Lead PI: <strong>Jiajia Li</strong>; PIs: Frank Mueller (NCSU), Dong Li (UC Merced), Lizhong Chen (OSU) <br/>
                    NSF PPoSS project, 09/07/2023 – 08/31/2028, Total amount: $5,000,000
                </p>
            </li> 
            <li>
                <header>
                    <p><em>CROSS: <a href="https://www.nsf.gov/awardsearch/showAward?AWD_ID=2247309&HistoricalAwards=false">Collaborative Research: PPoSS: Planning: Cross-layer Coordination and Optimization for Scalable and Sparse Tenor Networks(CROSS)</a>”</em></p>
                </header>
                <p>
                    Lead PI: <strong>Jiajia Li</strong>; PIs: Frank Mueller (NCSU), Dong Li (UC Merced), Lizhong Chen (OSU) <br/>
                    NSF PPoSS project, 10/01/2022 – 09/30/2024, Total amount: $250,000
                </p>
            </li> 

    </article>
</section>


<section class="twelve columns" id="publication">
    <h1>Publication</h1>
    <article>
        <ul>
            <li>
                <header>
                    <p><em> FlexMem: Adaptive Page Profiling and Migration for Tiered Memory.</em></p>
                    <span>Dong Xu, Junhee Ryu, Jinho Baek, Kwangsik Shin, Pengfei Su, and Dong Li.</span>
                </header>
                <p>
                    <em>30th USENIX Annual Technical Conference. <time>2024</time> <a href="https://pasalabs.org/papers/2024/ATC24_FlexMem.pdf">[pdf]</a> </em>
                </p>
            </li>
             <li>
                <header>
                    <p><em> MTM: Rethinking Memory Profiling and Migration for Multi-Tiered Large Memory Systems.</em></p>
                    <span>Jie Ren, Dong Xu, Junhee Ryu, Kwangsik Shin, Daewoo Kim, and Dong Li.</span>
                </header>
                <p>
                    <em>European Conference on Computer Systems. <time>2024</time> <a href="https://pasalabs.org/papers/2024/Eurosys24_M3_Camera_Ready.pdf">[pdf]</a> </em>
                </p>
            </li>
            <li>
                <header>
                    <p><em>Enabling Large Dynamic Neural Network Training with Learning-based Memory Management.</em></p>
                    <span>Jie Ren, Dong Xu, Shuangyan Yang, Jiacheng Zhao, Zhicheng Li, Christian Navasca, Chenxi Wang, Harry Xu, and Dong Li.</span>
                </header>
                <p>
                    <em>30th International Symposium on High-Performance Computer Architecture(acceptance rate: 18%).<time>2024</time> <a href="https://pasalabs.org/papers/2024/hpca24_dynn-offload.pdf">[pdf]</a> </em>
                </p>
            </li>
            <li>
                <header>
                    <p><em> Efficient Tensor Offloading for Large Deep-Learning Model Training based on Compute Express Link.</em></p>
                    <span>Dong Xu, Yuan Feng, Kwangsik Shin, Daewoo Kim, Hyeran Jeon, and Dong Li.</span>
                </header>
                <p>
                    <em>30th USENIX Annual Technical Conference (acceptance rate: 15.7%). <time>2024</time> <a href="https://pasalabs.org/papers/2024/tensor_offloading_cxl.pdf">[pdf]</a> </em>
                </p>
            </li>
            <li>
                <header>
                    <p><em>FASTEN: Fast GPU-accelerated Segmented Matrix Multiplication for Heterogenous Graph Neural Networks</em></p>
                    <span>Keren Zhou, Karthik Ganapathi Subramanian, Po-Hsun Lin, Matthias Fey, Binqian Yin, Jiajia Li.</span>
                </header>
                <p>
                    <em>The 38th ACM International Conference on Supercomputing (ICS). <time>2024</time> </em>
                </p>
            </li>
            <li>
                <header>
                    <p><em>DiaQ: Efficient State-Vector Quantum Simulation</em></p>
                    <span>Srikar Chundury, Jiajia Li, In-Saeng Suh and Frank Mueller.</span>
                </header>
                <p>
                    <em>ArXiv. <time>2024</time> </em>
                </p>
            </li>
            <li>
                <header>
                    <p><em>Parallel Sparse Tensor-times-Vector on Cerebras
WSE-2</em></p>
                    <span>Sai Krishna Teja Varm Manthena.</span>
                </header>
                <p>
                    <em>M.S. Thesis, North Carolina State University. <time>2024</time> </em>
                </p>
            </li>
            <li>
                <header>
                    <p><em>DiaQ: A Novel Quantum-Tailored Numerical Format</em></p>
                    <span>Srikar Chundury.</span>
                </header>
                <p>
                    <em>M.S. Thesis, North Carolina State University. <time>2024</time> </em>
                </p>
            </li>
            <li>
                <header>
                    <p><em>A PEPS plugin for TNQVM</em></p>
                    <span>Srikar Chundury, J. Lietz, E. A. C. Perez, A. Shehata, In-Saeng Suh and Frank Mueller.</span>
                </header>
                <p>
                    <em>IEEE International Conference on Quantum Computing and Engineering (QCE). <time>2023</time> </em>
                </p>
            </li>
        
        </ul>
    </article>
</section>


<section class="twelve columns" id="description">
    <h1>Description</h1>
        <article>

        <p>High-dimensional data computation and analytics are gaining importance in various domains, such as quantum chemistry/physics, quantum circuit simulation, social networks, healthcare, and machine/deep learning. Tensors, a representation of high-dimensional data, have become increasingly crucial. While extensive research has focused on tensor methods like decompositions and factorizations for low-dimensional data, there is a notable lack of development in tensor networks that cater to high-dimensional data (over ten dimensions) and can extract physically meaningful latent variables. The challenges arise from their complicated mathematical nature, extremely high computational complexity, and domain-specific difficulties. This project aims to bridge this critical gap by devising efficient tensor networks, especially for sparse data, which are prevalent in many real-world applications. The impacts of the project encompass four aspects: 1) Improving data compression, computation, memory usage, and interpretability of tensor networks; 2) fostering enduring and collaborative partnerships among academia, national research labs, and industry with a shared focus on the aforementioned applications; and 3) broadening education avenues by designing relevant new courses, training undergraduate and graduate students, organizing workshops, and enhancing K-12 outreach.
        </p></br>

        <p>This project delves into Cross-layer cooRdination and Optimization for Scalable and Sparse Tensor Networks (CROSS) designed for heterogeneous systems equipped with diverse accelerators like Graphics Processing Units (GPUs), Tensor Processing Units (TPUs) and Field Programmable Gate Arrays (FPGAs), and various memories such as dynamic and non-volatile random-access memories. This research aims to study sparsity within widely used tensor networks by incorporating constraints, regularization, dictionary, and domain knowledge. In addition to sparsity challenges, sparse tensor networks also face problems such as dimensionality, exacerbated data randomness and irregular program and memory access behaviors. This research tackles these challenges from four dimensions: (1) memory heterogeneity-aware representations and data (re-)arrangement, (2) balanced sparse tensor contraction algorithms with smart page arrangement, (3) memoization and intelligent allocation to reduce computational cost, and (4) specialized accelerator architectures for sparse tensor networks. The optimized sparse tensor networks represent a synergistic effort combining expertise from high-performance computing, algorithms, compilers, computer architecture and performance modeling. The proposed solutions are evaluated under diverse application scenarios and across a wide range of hardware environments to demonstrate their effectiveness and applicability in real-world settings.
        </p><br/>

    </article>
</section>

<section class="twelve columns" id="team">
    <h1>Team</h1>
    <article>
        <ul>
            <li>
                <header>
                    <a href="https://fruitfly1026.github.io/">Dr. Jiajia Li @ NCSU</a>
                </header>
            </li> 
            <li>
                <header>
                    <a href="https://arcb.csc.ncsu.edu/~mueller/">Dr. Frank Mueller @ NCSU</a>
                </header>
            </li> 
            <li>
                <header>
                    <a href="https://faculty.ucmerced.edu/dong-li/">Dr. Dong Li @ UC Merced</a>
                </header>
            </li>
            <li>
                <header>
                    <a href="https://web.engr.oregonstate.edu/~chenliz/">Dr. Lizhong Chen @ Oregon State</a>
                </header>
            </li> 
            <li>
                <header>
                    Zecheng Li @ NCSU
                </header>
            </li> 
            <li>
                <header>
                    Zhaonan Meng @ NCSU
                </header>
            </li> 
            <li>
                <header>
                    Teja Manthena @ NCSU
                </header>
            </li> 
            <li>
                <header>
                    Srikar Chundury @ NCSU
                </header>
            </li> 
            <li>
                <header>
                    Swastik Mittal @ NCSU
                </header>
            </li> 
            <li>
                <header>
                    Blake Burgstahler @ NCSU
                </header>
            </li>
            <li>
                <header>
                    Dong Xu @ UC Merced
                </header>
            </li> 
            <li>
                <header>
                    Gabriel Kulp @ Oregon State
                </header>
            </li> 
            <li>
                <header>
                    Andrew Ensinger @ Oregon State
                </header>
            </li> 
            <li>
                <header>
                    Raymond Baartmans @ Oregon State
                </header>
            </li> 

    </article>
</section>


<section class="twelve columns" id="contact">
    <h1>Contact</h1>
        <article>

        <p>
        <strong>Please feel free to drop me an email @ <a href="mailto:jiajia.li@ncsu.edu">jiajia.li@ncsu.edu</a> if you are interested in this project for collaboration.
        </p></strong><br/>

    </article>
</section>




<!-- <section class="twelve columns" id="artifacts">
    <h1>Software</h1>
    <article>
        <ul>
            <li>
                <header>
                    <h1><a href="https://github.com/pnnl/HiParTI">HiParTI</a></h1>
                </header>
                <p>
                    <em>A Hierarchical Parallel Tensor Infrastructure</em>
                </p>
            </li>
            <li>
                <header>
                    <h1><a href="https://gitlab.com/tensorworld/pasta">PASTA</a></h1>
                </header>
                <p>
                    <em>A Parallel Sparse Tensor Algorithm Benchmark Suite</em>
                </p>
            </li>
            <li>
                <header>
                    <h1><a href="https://github.com/hpcgarage/ParTI">ParTI</a></h1>
                </header>
                <p>
                    <em>A Parallel Tensor Infrastructure for Data Analysis</em>
                </p>
            </li>
            <li>
                <header>
                    <h1><a href="https://github.com/hpcgarage/AdaTM">AdaTM</a></h1>
                </header>
                <p>
                    <em>Adaptive Tensor Memoization algorithm for CP decomposition</em>
                </p>
            </li>
            <li>
                <header>
                    <h1><a href="https://github.com/hpcgarage/InTensLi">InTensLi</a></h1>
                </header>
                <p>
                    <em>Input-adaptive and in-place dense tensor-times-matrix multiply</em>
                </p>
            </li>
            <li>
                <header>
                    <h1><a href="https://gitlab.com/fruitfly1026/smat">SMAT</a></h1>
                </header>
                <p>
                    <em>Sparse Matrix-vector multiplication Auto-Tuner</em>
                </p>
            </li>
            <li>
                <header>
                    <h1><a href="http://www.ncic.ac.cn/~tgm/dgemm/dgemm_ati.html">HDGEMM</a></h1>
                </header>
                <p>
                    <em>A Hybrid DGEMM library on a Heterogeneous CPU-AMD GPU Architecture</em>
                </p>
            </li>
        </ul>
    </article>
</section>
 -->





    <footer class="twelve columns">
        <p>© All rights reserved. Powered by <a href="http://jekyllrb.com/">Jekyll and <a href="https://github.com">GitHub Pages</a></p>
    </footer>
    </div>
</div>

	
</body>
</html>